<?xml version='1.0' encoding='UTF-8'?>
<!DOCTYPE html PUBLIC '-//W3C//DTD XHTML+RDFa 1.1//EN' 'http://www.w3.org/MarkUp/DTD/xhtml-rdfa-2.dtd'>
<html dir="ltr" about="" property="dcterms:language" content="en" xmlns="http://www.w3.org/1999/xhtml" prefix='bibo: http://purl.org/ontology/bibo/' typeof="bibo:Document">
<head>
        <title>Intelligent Personal Assistant Architecture</title>
        <meta http-equiv="Content-Type" content="text/html;charset=utf-8">

        <link href="cg-draft.css" rel="stylesheet" type="text/css" charset="utf-8">

    </head>
    <body><div class="head">
            <p><a href="http://www.w3.org/">
                    <img width="72" height="48" src="http://www.w3.org/Icons/w3c_home" alt="W3C"></a></p>
            <h1 property="dcterms:title" class="title" id="title">Intelligent Personal Assistant Architecture</h1>
            <h2 property="bibo:subtitle" id="subtitle">Architecture and Potential for Standardization</h2>
            <dl>
                <dt>Latest version</dt>
                <dd>Last modified: January 31, 2020 <a href="https://github.com/w3c/voiceinteraction/blob/master/voice%20interaction%20drafts/paArchitecture.htm">https://github.com/w3c/voiceinteraction/blob/master/voice%20interaction%20drafts/paArchitecture.htm</a> (GitHub repository) </dd>
				<dd><a href ="https://w3c.github.io/voiceinteraction/voice%20interaction%20drafts/paArchitecture.htm">HTML rendered version</a></dd>             
			    <dt>Editors</dt>
                <p><span style="color: rgb(51, 51, 51); font-family: Conv_DroidSerif-Regular, serif; font-size: 15.7072px; orphans: 2; text-align: center; widows: 2; background-color: rgba(255, 255, 255, 0.6);">Dirk Schnelle-Walka</br>
									Deborah Dahl, Conversational Technologies</span></p>
								<p><span style="color: rgb(51, 51, 51); font-size: 15.7072px; orphans: 2; text-align: center; widows: 2; background-color: rgba(255, 255, 255, 0.6);"</p>
							
            </dl>
            <p class="copyright">Copyright © 2019 the Contributors to the Voice Interaction Community Group, 
                published by the  <a href="http://www.w3.org/community/voiceinteraction/">Voice Interaction Community Group</a> 
                under the <a href="https://www.w3.org/community/about/agreements/cla/">W3C Community Contributor License Agreement (CLA)</a>. A human-readable <a href="http://www.w3.org/community/about/agreements/cla-deed/">summary</a> is available.</p>
            <hr></div>

        <h2 id="abstract">Abstract</h2>

        <p>This documents describes a general architecture of Intelligent Personal Assistants and explores the potential for standardization.</p>

        <h2>Status of This Document</h2>

        <p><em>This specification was published by the 
                <a href="http://www.w3.org/community/voiceinteraction/">Voice Interaction Community Group</a>. 
                It is not a W3C Standard nor is it on the W3C Standards Track. 
                Please note that under the 
                <a href="http://www.w3.org/community/about/agreements/cla/">W3C&nbsp;</a></em><em><a href="http://www.w3.org/community/about/agreements/cla/">Community Contributor License Agreement (CLA)</a> there is a limited opt-out and other conditions apply. Learn more about <a href="http://www.w3.org/community/">W3C Community and Business Groups</a>.</em></p>

        <h2 class="introductory">Table of Contents</h2>
        <ul>
            <li><a href="#introduction">Introduction</a></li>
            <li><a href="#problemStatement">Problem Statement</a></li>
            <li><a href="#architecture">Architecture</a></li>
            <li><a href="#walkthrough">Use Case Walk Through</a></li>
            <li><a href="#potential">Potential for Standardization</a></li>
        </ul>

        <!-- OddPage -->
        <h2 id="introduction"><span class="secno">1. </span>Introduction</h2>
        <p>Intelligent Personal Assistants (IPA)s are already available in our daily lives through our smart phones. Apple’s Siri, Google Assistant, Microsoft’s Cortana, Samsung’s Bixby and 
		many more are helping us on various tasks, like shopping, playing music, setting schedule, sending messages, and offering answers to simple questions. Additionally, we equip our households
		with smart speakers like Amazon’s Alexa or Google Home to be available without the need to pick up explicit devices for this sort of tasks or even control household appliances in our homes.
		As of today, there is no interoperability between the available IPA providers. Especially for exchanging learned user behaviors this is unlikely to happen at all.</p>
		
		<p>This document describes a general architecture of IPAs and explores the potential areas for standardization. </p>
		
        <h2 id="problemStatement"><span class="secno">2. </span>Problem Statement</h2>
        
        <h3><span class="secno">2.1 Use Cases</span></h3>
				<p>This section describes potential usages of IPAs.</p> 
				
				<h4><span class="secno">2.1.1 </span><font face="Segoe UI">Travel Planning</font></h4>
				<p>A user would like to plan a trip to an international conference and she needs visa information and airline reservations. She will give the intelligent personal assistant her
				visa information (her citizenship, where she is going, purpose of travel, etc.) and it will respond by telling her the documentation she needs, how long the process will take
				and what the cost will be. This may require the personal assistant to consult with an auxiliary web service or another personal assistant that knows about visas.</p>

				<p>Once the user has found out about the visa, she tells the PA that she wants
				to make airline reservations. She specifies her dates of travel and airline preferences and the PA then interacts with her to find appropriate flights. </p>

				<p>A similar process will be repeated if the user wants to book a hotel, find
				a rental car, or find out about local attractions in the destination city.
				Booking a hotel as part of attending a conference could also involve finding out about a designated conference hotel or special conference rates. </p>
				
<h2 id="architecture"><span class="secno">3. </span></span><font face="Segoe UI">Architecture</font></span></h2>

		<p>In order to cope with such use cases as described above, an IPA may need to make use of several services descibing the capabilities of the IPA. These services may be selected from a
		standardized market place. For the reminder of the document, we consider an IPA that is extendable via such a market place. This kind of IPA features the architectural buildings blocks as
		shown in the following figure.</p>
		
		<img src="IPA-Architecture.svg" alt="IPA Architecture" style="width: 100%; height: auto;"/>

		<p>This architecture comprises 3 layers that are detailed in the following sections</p>
        <ol>
            <li><a href="#clientlayer">Client Layer</a></li>
            <li><a href="#dialoglayer">Dialog Layer</a></li>
            <li><a href="#datalayer">APIs / Data Layer</a></li>
        </ol>
		
        <h3 id="clientlayer"><span class="secno">3.1 Client Layer</span></h3>

        <h4 id="client"><span class="secno">3.1.1 </span>IPA Client</h4>
		<p>Client that enables the user to access the IPA via voice. Usually, IPA Clients make use of a microphone to capture the spoken input and a loud speaker to provide responses.
		As an extension an IPA Client may also receive commands to be executed locally.</p>

        <h3 id="dialoglayer"><span class="secno">3.2 Dialog Layer</span></h3>

        <h4><span class="secno">3.2.1 </span>IPA Service</h4>
		<p>General IPA Service API that mediates between the user and the overall IPA system. The service layer may be omitted in case the IPA Client communicates directly with the 
		<a href="#dialogmanagement">Dialog Management</a>.</p>

        <h4 id="dialogmanagement"><span class="secno">3.2.2 </span>Dialog Management</h4>
		<p>Component that receives user input as spoken input, updates its internal state, decides upon subsequent steps to continue a dialog and provides output as synthesized or recorded utterances.
		As an extension, it may also provide commands as output to be executed by the <a href="#client">IPA Client</a></p>
		<p>For this, it employs several Dialogs that are responsible to handle isolated tasks or intents. The following types of dialogs exist:</p>
		<ul>
			<li><a href="#coredialog">Core Dialog</a></li>
			<li><a href="#dialogx">Dialog X</a></li>
		</ul>
		<p>The overall set of available dialogs defines the behavior and capabilities of the interaction with the IPA. The Dialog Manager is also responsible for a good user experience across the available Dialogs.</p>
		<p>The Dialog Manager determines the Dialog that is best suited to serve the current user input and re-establishes the interaction state for that Dialog. Therefore, it may use the Dialog Registry.</p>
		<p>The Dialog Manager also manages the session with a user. Conceptually, multiple sessions can be active in parallel. Dialogs are governed by Sessions, e.g. to free resources of ASR and NLU engines when
		a session expires. Linguistic phenomena, like anaphoric references and ellipsis are expected to work within a Session. The selected IPA Provider or the Dialog Manager may have leading roles for this task.</p>
		<p>The Dialog Manager also features an <a href="#asr">ASR</a> to convert spoken utterances into text strings and a <a href="#tts">TTS</a> to convert text strings into audio.</p>
		<p>As an extension the Dialog Manager may also return commands to be executed by the IPA Client.</p>

        <h4 id="asr"><span class="secno">3.2.3 </span>ASR</h4>
		<p>The Automated Speech Recognizer (ASR) receives audio files of recorded utterances and generates a recognition hypothesis as text strings. Optionally, the ASR can also generate multiple recognition
		hypothesis along with a confidence score.</p>

        <h4 id="tts"><span class="secno">3.2.4 </span>TTS</h4>
		<p>The Text-to-Speech (TTS) component receives text strings, which it converts into audio data.</p>

        <h4 id="coredialog"><span class="secno">3.2.5 </span>Core Dialog</h4>
		<p>The Core Dialog is able to handle basic functionality via <a href="coreintesets">Core Intent Sets</a> to enable interaction with the user at all. This includes among others</p>
		<ul>
			<li>Greetings</li>
			<li>Goodbye</li>
			<li>Exception handling in case a requested service is not available</li>
			<li>Exception handling in case a requested intent cannot be matched to a known Dialog</li>
			<li>Help</li>
		</ul>
		<p>Conceptually, the Core Dialog is a special <a href="dialogx">Dialog</a> as described in the following section that is always available</p>
        <h5 id="dialog"><span class="secno">3.2.5.1 </span>Dialog</h5>
		<p>A Dialog is able to handle functionality that can be added to the capabilities of the <a href="#dialogmanagement">Dialog Management</a> through their associated Intent Sets. 
		The Dialogs must server different purpose in a sense that they are unique for a certain task. E.g., only a single flight reservation dialog may exist at a time.</p>
		<p>Dialogs receive inputs as intents out of their supported Intent sets along with associated entities and return responses as text strings to be spoken.</p>
		<p>As an extension Dialogs may also return commands to be executed by the <a href="#client">IPA Client</a>.</p>
		<p>Dialogs access the Provider Selection Service to fulfill their task. They maintain state which they also share with the <a href="#dialogmanagement">Dialog Management</a> and know which
		<a href="#provider">IPA Provider</a> evaluated their request with the help of an identifier.</p>

        <h4 id="coreintentsets"><span class="secno">3.2.6 </span>Core Intent Sets</h4>
		<p>A Core Intent Set usually identifies tasks to be executed and define the capabilities of the Core Dialog. Conceptually, the Core Intents are always available.</p>

        <h4 id="dialogx"><span class="secno">3.2.7 </span>Dialog X</h4>
		<p>The Dialog x are able to handle functionality that can be added to the capabilities of the Dialog Manager through their associated Intent Sets. The Dialogs must server different purpose
		in a sense that they are unique for a certain task. E.g., only a single flight reservation dialog may exist at a time.</p>
		<p>Dialogs receive inputs as intents out of their supported Intent sets along with associated entities and return responses as text strings to be spoken.</p>
		<p>Dialogs access the Provider Selection Service to fulfill their task. They maintain state and know which IPA Provider evaluated their request with the help of an identifier.</p>
		<p>As an extension Dialogs may also return commands to be executed by the IPA Client.</p>

        <h4><span class="secno">3.2.9 </span>Intent Set X</h4>
		<p>An Intent Set usually identifies tasks that can be executed within the associated Dialog.</p>
		
        <h4><span class="secno">3.2.10 </span>Dialog Registry</h4>
		<p>The Dialog registry manages all available Dialogs with their associated Intent Sets. Dialogs and their Intent Sets can be added or removed as needed.</p>
		
        <h3 id="datalayer"><span class="secno">3.3 APIs/Data Layer</span></h3>
		
        <h4><span class="secno">3.3.1 </span>Provider Selection Service</h4>
		<p>A service that provides access to all known IPA Providers. This service also maps the IPA Intent Sets to the Intent Sets in the Dialog layer.</p>
		<p>The Provider Selection Service receives input as text strings and returns results as intents from all IPA Providers that are able to reply to the user input along with associated entities.</p>
		<p>The Provider Selection Service is stateless and always returns the responses from the IPA Providers along with an identification of the issuing IPA Provider.</p>

        <h4><span class="secno">3.3.2 </span>Accounts/Autenthication</h4>
		<p>A registry that knows how to access the known IPA Providers, i.e. which are available and credentials to access them. Storing of credentials must meet security and trust considerations that are expected from such a personalized service. IPA Providers can be added as needed.</p>

        <h4><span class="secno">3.3.3 </span>Core NLU</h4>
		<p>A component that is able to extract meaning as intents and associated entities from an utterance as text strings.</p>
		<p>The Core NLU is able to handle basic functionality via Core Intents to enable interaction with the user at all.</p>

        <h4><span class="secno">3.3.4 </span>Core Data Provider</h4>
		<p>A generic data provider to aid the Core NLU determining the intent.</p>
		
        <h4 id="provider"><span class="secno">3.3.5 </span>IPA Provider X</h4>
		<p>A provider of an IPA service, like</p>
		<ul>
			<li>Google Assistant</li>
			<li>Alexa</li>
			<li>Microsoft Cortana</li>
			<li>SoundHound</li>
			<li>&#x2026;</li>
		</ul>

        <h5><span class="secno">3.3.5.1 </span>Provider NLU</h5>
		<p>A component that is able to extract meaning as intents and associated entities from an utterance as text strings for IPA Provider X</p>

        <h5><span class="secno">3.3.5.2 </span>Provider Intent Set</h5>
		<p>An intent set that might be returned by Provider NLU to handle the capabilities of Provider X.</p>

        <h5><span class="secno">3.3.5.3 </span>Data Provider</h5>
		<p>A data provider to aid the Provider NLU in determining the intent. This component may also be used to track the IPA Provider’s state

        <h5><span class="secno">3.3.5.4 </span>Knowledge Graph</h5>
		<p>A knowledge graph to reason about the detected input from the Provider NLU and Data Provider to come up with some more meaningful results.</p>
		

        <h2 id="walkthrough"><span class="secno">4. </span>Use Case Walk Through</h2>

        <p>This section expands on the use case above, filling in details according to the sample architecture.</p>
        <p>A user would like to plan a trip to an international conference and she
needs visa information and airline reservations. </p>

<p>The user starts by asking a general purpose assistant ("IPA Service", on the left
of the diagram) about what the visa requirements are for her situation. For
a common situation, such as citizens of the EU traveling to the United
States, the IPA is able to answer the question directly by getting the
information from a web service that it knows about. This could be one of its
dialogs 1-n. However, for less common situations (for example, a citizen of
South Africa traveling to Japan), the generic IPA will try to identify a
visa expert assistant application from the dialog registry. If it finds one,
it will connect the user with the visa expert, one of the IPA providers on
the right side. The visa expert will then engage in a dialog with the user
to find out the dates and purposes of travel and will inform the user of the
visa process. </p>

<p>Once the user has found out about the visa, she tells the IPA that she wants
to make airline reservations. If she wants to use a particular service, or
use a particular airline, she would say something like "I want to book a
flight on American". The IPA will then either connect the user with
American's IPA or, if American doesn't have an IPA, will inform the user of
that fact. On the other hand, if the user doesn't specify an airline, the
IPA will find a general flight search IPA from its registry and connect the
user with the IPA for that flight search service.  The flight search IPA
will then interact with the user to find appropriate flights. </p>

<p>A similar process would be repeated if the user wants to book a hotel, find
a rental car, find out about local attractions in the destination city, etc.
Booking a hotel could also involve interacting with the conference's IPA to
find out about a designated conference hotel or special rates. 
</p>

        <h2 id="potential"><span class="secno">5. </span>Potential for Standardization</h2>

        <p>t.b.d.</p>

</body>
</html>